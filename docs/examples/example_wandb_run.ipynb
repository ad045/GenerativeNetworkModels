{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Script for Running a Sweep with WandB logging\n",
    "\n",
    "<p> Wandb (Weights and Biases) is a service that evaluates and saves model parameters while providing excellent visualisation tools - ideal for running experiments. Here, you'll see how you can use wandb in tandem with the GNM toolbox </p>\n",
    "\n",
    "<p><i>Wandb is a seperate service not affiliated with this toolbox - for wandb-specific support, have a look at their documentation.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/adrian/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrian-dendorfer\u001b[0m (\u001b[33madrian_s_playground\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic imports from the package with torch and numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from gnm.fitting.experiment_saving import *\n",
    "from gnm.fitting.experiment_dataclasses import Experiment\n",
    "from gnm import defaults, fitting, generative_rules, weight_criteria, evaluation\n",
    "\n",
    "# import wandb - run 'pip install wandb' if it's not already installed\n",
    "import wandb\n",
    "\n",
    "# Use the correct Device - this is CPU or GPU (use GPU if you have one to utilize parallelization\n",
    "# which will speed things up considerably)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the basic matrices - here, we'll just use a binary network\n",
    "distance_matrix = defaults.get_distance_matrix(device=DEVICE)\n",
    "binary_consensus_network = defaults.get_binary_network(device=DEVICE)\n",
    "\n",
    "# login using wandb - you'll need to create an account if you\n",
    "# dont have one already \n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the basic parameters - we'll iterate through just 4 combinations \n",
    "# here for demonstration purposes, but you can set this to any number\n",
    "\n",
    "eta_values = torch.Tensor([1, 1.5]) #torch.linspace(-5, -1, 1)\n",
    "gamma_values = torch.Tensor([-1, -0.5])#torch.linspace(-0.5, 0.5, 1)\n",
    "num_connections = int( binary_consensus_network.sum().item() / 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the basic and for the most part, default parameters you'd use in a run. \n",
    "# Have a look at the other example scripts for an in-depth look at the parameters and how you can \n",
    "# use them yourself.\n",
    "\n",
    "# The binary sweep parameters are the parameters that are used to generate the binary network\n",
    "binary_sweep_parameters = fitting.BinarySweepParameters(\n",
    "    eta = eta_values,\n",
    "    gamma = gamma_values,\n",
    "    lambdah = torch.Tensor([0.0]),\n",
    "    distance_relationship_type = [\"powerlaw\"],\n",
    "    preferential_relationship_type = [\"powerlaw\"],\n",
    "    heterochronicity_relationship_type = [\"powerlaw\"],\n",
    "    generative_rule = [generative_rules.MatchingIndex()],\n",
    "    num_iterations = [num_connections],\n",
    ")\n",
    "\n",
    "# The weighted sweep parameters are the parameters that are used to generate the weighted network\n",
    "weighted_sweep_parameters = fitting.WeightedSweepParameters(\n",
    "    alpha = [0.01],\n",
    "    optimisation_criterion = [weight_criteria.DistanceWeightedCommunicability(distance_matrix=distance_matrix) ],\n",
    ")  \n",
    "\n",
    "\n",
    "# The sweep config is the object that contains all the parameters for the sweep\n",
    "# and is used to generate the networks.\n",
    "sweep_config = fitting.SweepConfig(\n",
    "    binary_sweep_parameters = binary_sweep_parameters,\n",
    "    weighted_sweep_parameters = weighted_sweep_parameters,\n",
    "    num_simulations = 1,\n",
    "    distance_matrix = [distance_matrix]    \n",
    ")\n",
    "\n",
    "# additonal cirteria to evaluate the generative model against a real connectome\n",
    "criteria = [ evaluation.ClusteringKS(), evaluation.DegreeKS(), evaluation.EdgeLengthKS(distance_matrix) ]\n",
    "energy = evaluation.MaxCriteria( criteria )\n",
    "binary_evaluations = [energy]\n",
    "weighted_evaluations = [ evaluation.WeightedNodeStrengthKS(normalise=True), evaluation.WeightedClusteringKS() ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: None for GNM simulations\n",
      "Logging experiment to wandb - login may be required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration Iterations:   0%|          | 0/4 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'generative_model_experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#  Run the experment sweep. You should see a wandb link in the terminal. Follow this\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# link to see the model parameters visualized in the wandb dashboard. This will make \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# parameter combinations easier to visualize and understand.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m experiments = \u001b[43mfitting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43msweep_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mbinary_evaluations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_evaluations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mreal_binary_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_consensus_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mweighted_evaluations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweighted_evaluations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                \u001b[49m\u001b[43msave_run_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mwandb_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set this to true for logging, it's False by default\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ma_thesis/lib/python3.13/site-packages/jaxtyping/_decorator.py:483\u001b[39m, in \u001b[36mjaxtyped.<locals>.wrapped_fn_impl\u001b[39m\u001b[34m(args, kwargs, bound, memos)\u001b[39m\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m out = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_signature.return_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.Signature.empty:\n\u001b[32m    486\u001b[39m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[32m    500\u001b[39m     kwargs[output_name] = out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ma_thesis/lib/python3.13/site-packages/gnm/fitting/sweep.py:460\u001b[39m, in \u001b[36mperform_sweep\u001b[39m\u001b[34m(sweep_config, binary_evaluations, weighted_evaluations, real_binary_matrices, real_weighted_matrices, save_model, save_run_history, device, verbose, wandb_logging, method, num_bayesian_runs, metric_to_optimise)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    459\u001b[39m     config_count = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(sweep_config))\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     run_results, run_times = \u001b[43mperform_grid_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    463\u001b[39m     avg_time = \u001b[38;5;28msum\u001b[39m(run_times) / \u001b[38;5;28mlen\u001b[39m(run_times)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ma_thesis/lib/python3.13/site-packages/gnm/fitting/sweep.py:356\u001b[39m, in \u001b[36mperform_sweep.<locals>.perform_grid_sweep\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wandb_logging:\n\u001b[32m    355\u001b[39m     exp = ExperimentEvaluation(save=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     experiment_data_config = \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_save_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     wandb.init(project=project_name, config=experiment_data_config)\n\u001b[32m    358\u001b[39m     wandb.log(experiment_data_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ma_thesis/lib/python3.13/site-packages/gnm/fitting/experiment_saving.py:143\u001b[39m, in \u001b[36mExperimentEvaluation._save_experiment\u001b[39m\u001b[34m(self, experiment_dataclass, experiment_name)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(final_number)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m all_experiments = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m with_name = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m all_experiments \u001b[38;5;28;01mif\u001b[39;00m experiment_name \u001b[38;5;129;01min\u001b[39;00m i]\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(with_name) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'generative_model_experiments'"
     ]
    }
   ],
   "source": [
    "#  Run the experment sweep. You should see a wandb link in the terminal. Follow this\n",
    "# link to see the model parameters visualized in the wandb dashboard. This will make \n",
    "# parameter combinations easier to visualize and understand.\n",
    "experiments = fitting.perform_sweep(sweep_config=sweep_config, \n",
    "                                binary_evaluations=binary_evaluations, \n",
    "                                real_binary_matrices=binary_consensus_network,\n",
    "                                weighted_evaluations=weighted_evaluations,\n",
    "                                save_model = False,\n",
    "                                save_run_history = False,\n",
    "                                verbose=True,\n",
    "                                wandb_logging=True # Set this to true for logging, it's False by default\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
