{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNM Experiment: Sweeping, Saving, and Querying\n",
    "\n",
    "This notebook demonstrates the process of setting up and running a parameter sweep for a generative network model (GNM) using the `gnm` library. We will:\n",
    "\n",
    "1.  **Configure Environment**: Set up the PyTorch device and load necessary data.\n",
    "2.  **Define Parameters**: Specify the parameter space for both binary and weighted network generation.\n",
    "3.  **Set Evaluation Criteria**: Define the metrics to evaluate the similarity between generated networks and a real-world consensus network.\n",
    "4.  **Run the Sweep**: Execute the experiment using `fitting.perform_sweep`.\n",
    "5.  **Save and Query**: Use the `ExperimentEvaluation` class to save the results and demonstrate how to query them based on specific parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "First, we import all the necessary modules from the `gnm` library and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnm.fitting.experiment_saving import ExperimentEvaluation\n",
    "from gnm.fitting.experiment_dataclasses import Experiment\n",
    "from gnm import defaults, fitting, generative_rules, weight_criteria, evaluation\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Data Loading\n",
    "\n",
    "We'll set the device to a GPU if available, otherwise, it will default to the CPU. We then load a pre-defined distance matrix and a binary consensus network, which will serve as the ground truth for our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "distance_matrix = defaults.get_distance_matrix(device=DEVICE)\n",
    "binary_consensus_network = defaults.get_binary_network(device=DEVICE)\n",
    "\n",
    "print(f\"Distance matrix shape: {distance_matrix.shape}\")\n",
    "print(f\"Binary consensus network shape: {binary_consensus_network.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Sweep Parameters\n",
    "\n",
    "Here, we define the parameter space for our sweep. This is broken down into two parts:\n",
    "\n",
    "* `BinarySweepParameters`: Defines the parameters for generating the network topology (the binary connections). We specify values for `eta` and `gamma`, the generative rule (`MatchingIndex`), and relationship types.\n",
    "* `WeightedSweepParameters`: Defines the parameters for assigning weights to the connections, including the `alpha` parameter and the optimization criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we use single values. To perform a wider sweep, \n",
    "# you could use torch.linspace or a list of values.\n",
    "eta_values = torch.Tensor([1])\n",
    "gamma_values = torch.Tensor([-1])\n",
    "\n",
    "# Calculate the number of connections to generate\n",
    "num_connections = int(binary_consensus_network.sum().item() / 2)\n",
    "\n",
    "binary_sweep_parameters = fitting.BinarySweepParameters(\n",
    "    eta=eta_values,\n",
    "    gamma=gamma_values,\n",
    "    lambdah=torch.Tensor([0.0]),\n",
    "    distance_relationship_type=[\"powerlaw\"],\n",
    "    preferential_relationship_type=[\"powerlaw\"],\n",
    "    heterochronicity_relationship_type=[\"powerlaw\"],\n",
    "    generative_rule=[generative_rules.MatchingIndex()],\n",
    "    num_iterations=[num_connections],\n",
    ")\n",
    "\n",
    "weighted_sweep_parameters = fitting.WeightedSweepParameters(\n",
    "    alpha=[0.01],\n",
    "    optimisation_criterion=[weight_criteria.DistanceWeightedCommunicability(distance_matrix=distance_matrix)],\n",
    ")\n",
    "\n",
    "print(\"Binary Sweep Parameters:\")\n",
    "print(binary_sweep_parameters)\n",
    "print(\"\\nWeighted Sweep Parameters:\")\n",
    "print(weighted_sweep_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Sweep Configuration\n",
    "\n",
    "The `SweepConfig` object combines the binary and weighted parameters with general simulation settings, such as the number of simulations to run for each parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 1\n",
    "\n",
    "sweep_config = fitting.SweepConfig(\n",
    "    binary_sweep_parameters=binary_sweep_parameters,\n",
    "    weighted_sweep_parameters=weighted_sweep_parameters,\n",
    "    num_simulations=num_simulations,\n",
    "    distance_matrix=[distance_matrix]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining Evaluation Criteria\n",
    "\n",
    "We need to define how to score the generated networks. We use Kolmogorov-Smirnov (KS) tests to compare the distributions of various network properties (clustering, degree, edge length) against the real network. The `MaxCriteria` function is used to select the maximum (worst) KS statistic among these as a single energy score for the binary model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [evaluation.ClusteringKS(), evaluation.DegreeKS(), evaluation.EdgeLengthKS(distance_matrix)]\n",
    "energy = evaluation.MaxCriteria(criteria)\n",
    "binary_evaluations = [energy]\n",
    "\n",
    "weighted_evaluations = [evaluation.WeightedNodeStrengthKS(normalise=True), evaluation.WeightedClusteringKS()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Experiment Sweep\n",
    "\n",
    "With all configurations in place, we call `fitting.perform_sweep` to run the simulations. This function iterates through all parameter combinations, generates networks, evaluates them, and returns a list of `Experiment` objects containing the results. We set `verbose=True` to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = fitting.perform_sweep(\n",
    "    sweep_config=sweep_config, \n",
    "    binary_evaluations=binary_evaluations, \n",
    "    real_binary_matrices=binary_consensus_network,\n",
    "    weighted_evaluations=weighted_evaluations,\n",
    "    save_model=False, # Set to True to save the model instances\n",
    "    save_run_history=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving and Querying Experiment Results\n",
    "\n",
    "Finally, we demonstrate how to manage the results. \n",
    "\n",
    "1.  Instantiate `ExperimentEvaluation`.\n",
    "2.  Use `.save_experiments()` to save the list of experiment objects. This will typically save to a file for later access.\n",
    "3.  Use `.query_experiments()` to filter and retrieve specific experiments from the saved set based on their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ExperimentEvaluation class handles saving and loading\n",
    "eval_handler = ExperimentEvaluation()\n",
    "\n",
    "# Save the list of experiments\n",
    "eval_handler.save_experiments(experiments)\n",
    "print(f\"Saved {len(experiments)} experiment(s).\")\n",
    "\n",
    "# Query the experiments by a binary parameter\n",
    "print(\"\\nQuerying for generative_rule = 'MatchingIndex'...\")\n",
    "query_by_rule = eval_handler.query_experiments(by='generative_rule', value='MatchingIndex')\n",
    "print(f\"Found {len(query_by_rule)} experiment(s).\")\n",
    "print(query_by_rule)\n",
    "\n",
    "# Query the experiments by a weighted parameter\n",
    "print(\"\\nQuerying for alpha = 0.01...\")\n",
    "query_by_alpha = eval_handler.query_experiments(by='alpha', value=0.01)\n",
    "print(f\"Found {len(query_by_alpha)} experiment(s).\")\n",
    "print(query_by_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}